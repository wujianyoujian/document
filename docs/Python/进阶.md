# 进阶
> 进阶知识

## 并发编程

### 多线程
> `threading`

* `python`中的多线程有锁, 一定程度上保证线程的安全性
1. 在部署到服务器上响应请求的, 不是用框架自带开启的服务器, 因为它是一个单进程单线程的应用
2. 就像node.js一样他本身是一个单线程的语言, `express`和`koa`开启的服务都是单进程和单线程
3. 假如有10个请求同时访问, 只会依次返回响应
* `werkzeug` 线程隔离的包
* <b>使用线程隔离的 意义在于：是当前线程能够正确引用到他自己所创建的对象, 而不是引用到其他线程所创建的对象</b>

```python
# 基本使用, 开启多线程爬取视频
import threading
from time import time

import blog


def single_thread():
    print("---single craw start---")
    for url in blog.urls:
        blog.craw(url)


def multi_thread():
    print("---multithread craw start---")
    threadList = []
    for url in blog.urls:
        threadList.append(
            threading.Thread(target=blog.craw, args=(url,))
        )
    for thread in threadList:
        thread.start()
    for thread in threadList:
        thread.join()


if __name__ == '__main__':
    # start = time()
    # single_thread()
    # end = time()
    # print("cost time: ", end - start, "seconds")

    start = time()
    multi_thread()
    end = time()
    print("cost time: ", end - start, "seconds")

```

#### queue
> 
```python
# 消费者生产者, 3个线程用于爬取网页, 2个线程用于解析网页内容
import queue
import random
import threading
import time

from blog import urls, parse, craw


def do_craw(url_queue: queue.Queue, html_queue: queue.Queue):
    while True:
        url = url_queue.get()
        html = craw(url)
        html_queue.put(html)
        print(threading.current_thread().name, f"craw {url}", "url_queue.size=", url_queue.qsize())
        time.sleep(random.randint(1, 2))


def do_parse(html_queue: queue.Queue, file):
    while True:
        html = html_queue.get()
        results = parse(html)
        for result in results:
            file.write(str(result) + "\n")
        print(threading.current_thread().name, f"result.size", len(results), "html_queue.size=", html_queue.qsize())
        time.sleep(random.randint(1, 2))


if __name__ == '__main__':
    url_queue = queue.Queue()
    html_queue = queue.Queue()
    for url in urls:
        url_queue.put(url)
    # 三个线程进行爬取
    for i in range(1, 4):
        thread = threading.Thread(target=do_craw, args=(url_queue, html_queue), name=f"craw_thread_{i}")
        thread.start()
    # 两个线程进行解析
    file = open('data.json', mode="w", encoding="utf-8")
    for i in range(1, 3):
        thread = threading.Thread(target=do_parse, args=(html_queue, file), name=f"parse_thread_{i}")
        thread.start()

```
#### lock
> 当多个线程对一个变量进行运行, 线程会进行切换, 同时读取更改这个变量, 发生错误, 使用`time.sleep()`肯定会发生错误, 因为`time.sleep`会引起线程的切换
* 使用`with`代替`try exception finally`

```python
lock = threading.Lock()

lock.acquire()
try:
  pass
except:
  pass
finally:
  lock.release()
```

```python
import threading
import time

lock = threading.Lock()


class Account:
    def __init__(self, balance):
        self.balance = balance


def with_draw_money(account: Account, money):
    with lock:
        if account.balance >= money:
            time.sleep(.1)
            account.balance -= money
            print("当前线程:", threading.current_thread().name, "取钱成功")
        else:
            print("取钱失败, 余额不足")


if __name__ == '__main__':
    account1 = Account(1000)
    threadList = []
    for i in range(2):
        t = threading.Thread(target=with_draw_money, args=(account1, 800), name=f't{i}')
        threadList.append(t)
        t.start()

    for t in threadList:
        t.join()

    print(f"当前余额{account1.balance}")

```
#### 线程池
* 新建大量线程和终止线程会消耗性能  
>  新建线程系统需要分配资源, 终止线程需要回收资源  
> 因此如果重新使用原来的线程, 可以减少性能的消耗  
?> 需要大量线程完成任务, 但是实际任务处理时间短, 避免线程过多, 导致系统负荷较大, 变慢卡顿等问题

```python
import concurrent.futures
import blog

# craw
with concurrent.futures.ThreadPoolExecutor() as pool:
    htmls = pool.map(blog.craw, blog.urls)
    htmls = list(zip(blog.urls, htmls))
    for url, html in htmls:
        print(url, len(html))

print("craw over")

# parse
with concurrent.futures.ThreadPoolExecutor() as pool:
    futures = {}
    for url, html in htmls:
        future = pool.submit(blog.parse, html)
        futures[future] = url

    # for future, url in futures.items():
    #    print(url, future.result())

    # as_completed 有了结果就返回
    for future in concurrent.futures.as_completed(futures):
        url = futures[future]
        print(url, future.result())

```

### 多进程
> `multiprocessing`
> 多核CPU进行运行
一个进程可以启动多个线程

### 多协程
> `asynic`

### GIL
> 多线程只能并发执行, 并不能利用多CPU, 某时刻只有一个线程执行
